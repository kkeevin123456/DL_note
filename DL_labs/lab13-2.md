* 此次 lab 只需要更動 loading data，CNN architecture 即可。
* learning rate : 控制 gradient descent 每一步要走多大步
* RNN 裡面 
" x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) "
 是把 context_vector 跟 input x 做內積之後得到新的 x
* GRU 的部分就包含了 time 的維度，如果想要自己實作可以呼叫 gru-cell 來自己實作每個 cell

### 注意:
	1. 如果 loss 一直降不下去就要把 optimizer 裡面 adam 的 learning rate 改小一點
	2. CNN 最後 output 的維度要跟 embedding_dim 一樣，因為他是要對接到 RNN 的 input 的